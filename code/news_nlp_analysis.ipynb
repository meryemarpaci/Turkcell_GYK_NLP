{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# News Headlines NLP Analysis\n",
        "\n",
        "This notebook implements a comprehensive NLP pipeline for analyzing news headlines, including:\n",
        "- Text preprocessing\n",
        "- Vectorization comparison\n",
        "- Sentiment analysis\n",
        "- Topic modeling\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Import Libraries\n",
        "\n",
        "First, let's import all necessary libraries for our analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('ggplot')\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Load and Explore Data\n",
        "\n",
        "The dataset is in JSON format. Since it may be large, we'll use a line-by-line loading approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    # Read JSON file line by line (more memory efficient for large files)\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            data.append(json.loads(line))\n",
        "            if i == 10000:  # Load a subset for demonstration purposes\n",
        "                break\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# File path\n",
        "file_path = \"archive/News_Category_Dataset_v3.json\"\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "df = load_data(file_path)\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(\"\\nColumns in the dataset:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore data further\n",
        "# Check category distribution\n",
        "category_counts = df['category'].value_counts()\n",
        "\n",
        "# Visualize top 10 categories\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=category_counts.values[:10], y=category_counts.index[:10])\n",
        "plt.title('Top 10 News Categories')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Category')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Text Preprocessing\n",
        "\n",
        "We'll implement several preprocessing steps in sequence:\n",
        "1. Tokenization\n",
        "2. Lowercasing\n",
        "3. Stopword removal\n",
        "4. Lemmatization\n",
        "5. Optional POS tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Apply all preprocessing steps in sequence\n",
        "    \"\"\"\n",
        "    # Step 1 & 2: Convert to lowercase (before tokenization so proper handling of case-sensitive tokens)\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove special characters and numbers (helps with tokenization)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    # Step 3: Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Step 4: Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Step 5: Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def apply_pos_tagging(text):\n",
        "    \"\"\"\n",
        "    Apply POS tagging to text\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    return tagged\n",
        "\n",
        "# Preprocess headlines\n",
        "print(\"Preprocessing headlines...\")\n",
        "df['processed_headline'] = df['headline'].apply(preprocess_text)\n",
        "\n",
        "# Display example of original vs processed headlines\n",
        "sample_headlines = df[['headline', 'processed_headline']].head(5)\n",
        "sample_headlines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration of POS tagging on a sample headline\n",
        "sample_headline = df['headline'].iloc[0]\n",
        "tagged_words = apply_pos_tagging(sample_headline)\n",
        "\n",
        "print(f\"Original headline: {sample_headline}\")\n",
        "print(\"\\nPOS tags:\")\n",
        "for word, tag in tagged_words:\n",
        "    print(f\"{word}: {tag}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Vectorization Methods\n",
        "\n",
        "We'll compare two common vectorization approaches:\n",
        "1. Bag of Words (CountVectorizer)\n",
        "2. TF-IDF Vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vectorize_bow(texts, max_features=5000):\n",
        "    \"\"\"\n",
        "    Vectorize using Bag of Words\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "    bow_matrix = vectorizer.fit_transform(texts)\n",
        "    return bow_matrix, vectorizer\n",
        "\n",
        "def vectorize_tfidf(texts, max_features=5000):\n",
        "    \"\"\"\n",
        "    Vectorize using TF-IDF\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "print(\"Applying vectorization methods...\")\n",
        "# Bag of Words\n",
        "bow_matrix, bow_vectorizer = vectorize_bow(df['processed_headline'])\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_matrix, tfidf_vectorizer = vectorize_tfidf(df['processed_headline'])\n",
        "\n",
        "print(f\"Bag of Words matrix shape: {bow_matrix.shape}\")\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare vectorization methods for a sample headline\n",
        "sample_idx = 0\n",
        "sample_headline_processed = df['processed_headline'].iloc[sample_idx]\n",
        "original_headline = df['headline'].iloc[sample_idx]\n",
        "\n",
        "print(f\"Original headline: {original_headline}\")\n",
        "print(f\"Processed headline: {sample_headline_processed}\")\n",
        "\n",
        "# Get BoW representation\n",
        "sample_bow = bow_vectorizer.transform([sample_headline_processed])\n",
        "sample_bow_array = sample_bow.toarray()[0]\n",
        "sample_bow_features = bow_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get TF-IDF representation\n",
        "sample_tfidf = tfidf_vectorizer.transform([sample_headline_processed])\n",
        "sample_tfidf_array = sample_tfidf.toarray()[0]\n",
        "sample_tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Show non-zero elements for both\n",
        "print(\"\\nBag of Words representation (non-zero elements):\")\n",
        "for idx in np.nonzero(sample_bow_array)[0][:10]:  # Show first 10 elements\n",
        "    print(f\"{sample_bow_features[idx]}: {sample_bow_array[idx]}\")\n",
        "\n",
        "print(\"\\nTF-IDF representation (non-zero elements):\")\n",
        "for idx in np.nonzero(sample_tfidf_array)[0][:10]:  # Show first 10 elements\n",
        "    print(f\"{sample_tfidf_features[idx]}: {sample_tfidf_array[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Sentiment Analysis\n",
        "\n",
        "We'll implement and compare two popular sentiment analysis approaches:\n",
        "1. TextBlob\n",
        "2. VADER (Valence Aware Dictionary and sEntiment Reasoner)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sentiment_textblob(text):\n",
        "    \"\"\"\n",
        "    Analyze sentiment using TextBlob\n",
        "    \"\"\"\n",
        "    analysis = TextBlob(text)\n",
        "    polarity = analysis.sentiment.polarity\n",
        "    \n",
        "    if polarity > 0.1:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def analyze_sentiment_vader(text):\n",
        "    \"\"\"\n",
        "    Analyze sentiment using VADER\n",
        "    \"\"\"\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    \n",
        "    if sentiment_scores['compound'] >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif sentiment_scores['compound'] <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "print(\"Performing sentiment analysis...\")\n",
        "\n",
        "# Use a sample of headlines for demonstration purposes\n",
        "sample_size = min(1000, len(df))\n",
        "sample_df = df.sample(sample_size, random_state=42)\n",
        "\n",
        "# TextBlob\n",
        "sample_df['sentiment_textblob'] = sample_df['headline'].apply(analyze_sentiment_textblob)\n",
        "\n",
        "# VADER\n",
        "sample_df['sentiment_vader'] = sample_df['headline'].apply(analyze_sentiment_vader)\n",
        "\n",
        "# Compare sentiment analysis results\n",
        "textblob_sentiments = sample_df['sentiment_textblob'].value_counts()\n",
        "vader_sentiments = sample_df['sentiment_vader'].value_counts()\n",
        "\n",
        "print(\"\\nTextBlob Sentiment Distribution:\")\n",
        "print(textblob_sentiments)\n",
        "\n",
        "print(\"\\nVADER Sentiment Distribution:\")\n",
        "print(vader_sentiments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sentiment analysis results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x=textblob_sentiments.index, y=textblob_sentiments.values)\n",
        "plt.title('TextBlob Sentiment Distribution')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x=vader_sentiments.index, y=vader_sentiments.values)\n",
        "plt.title('VADER Sentiment Distribution')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare sentiment analysis methods with examples\n",
        "print(\"Examples of headlines with different sentiment classifications:\")\n",
        "\n",
        "# Find examples where TextBlob and VADER disagree\n",
        "disagreement = sample_df[sample_df['sentiment_textblob'] != sample_df['sentiment_vader']].head(5)\n",
        "print(\"\\nDisagreement between TextBlob and VADER:\")\n",
        "for i, row in disagreement.iterrows():\n",
        "    print(f\"Headline: {row['headline']}\")\n",
        "    print(f\"TextBlob: {row['sentiment_textblob']}, VADER: {row['sentiment_vader']}\")\n",
        "    \n",
        "    # Show detailed scores\n",
        "    tb = TextBlob(row['headline'])\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    vader_scores = sid.polarity_scores(row['headline'])\n",
        "    \n",
        "    print(f\"TextBlob polarity: {tb.sentiment.polarity:.4f}\")\n",
        "    print(f\"VADER scores: {vader_scores}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Topic Modeling\n",
        "\n",
        "We'll implement two popular topic modeling approaches:\n",
        "1. Latent Dirichlet Allocation (LDA)\n",
        "2. Non-Negative Matrix Factorization (NMF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_lda(vector_matrix, vectorizer, num_topics=10):\n",
        "    \"\"\"\n",
        "    Perform LDA topic modeling\n",
        "    \"\"\"\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(vector_matrix)\n",
        "    \n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Extract topics\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words_idx = topic.argsort()[:-11:-1]  # Get top 10 words\n",
        "        top_words = [feature_names[i] for i in top_words_idx]\n",
        "        topics.append(top_words)\n",
        "        \n",
        "    return lda, topics\n",
        "\n",
        "def perform_nmf(vector_matrix, vectorizer, num_topics=10):\n",
        "    \"\"\"\n",
        "    Perform NMF topic modeling\n",
        "    \"\"\"\n",
        "    nmf = NMF(n_components=num_topics, random_state=42)\n",
        "    nmf.fit(vector_matrix)\n",
        "    \n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Extract topics\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(nmf.components_):\n",
        "        top_words_idx = topic.argsort()[:-11:-1]  # Get top 10 words\n",
        "        top_words = [feature_names[i] for i in top_words_idx]\n",
        "        topics.append(top_words)\n",
        "        \n",
        "    return nmf, topics\n",
        "\n",
        "# Perform topic modeling with a smaller number of topics for faster execution\n",
        "num_topics = 5\n",
        "\n",
        "print(\"\\nRunning LDA with TF-IDF...\")\n",
        "lda_model, lda_topics = perform_lda(tfidf_matrix, tfidf_vectorizer, num_topics=num_topics)\n",
        "\n",
        "print(\"\\nRunning NMF with TF-IDF...\")\n",
        "nmf_model, nmf_topics = perform_nmf(tfidf_matrix, tfidf_vectorizer, num_topics=num_topics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print topics from LDA\n",
        "print(\"\\nLDA Topics:\")\n",
        "for i, topic_words in enumerate(lda_topics):\n",
        "    print(f\"Topic {i+1}: {', '.join(topic_words)}\")\n",
        "\n",
        "# Print topics from NMF\n",
        "print(\"\\nNMF Topics:\")\n",
        "for i, topic_words in enumerate(nmf_topics):\n",
        "    print(f\"Topic {i+1}: {', '.join(topic_words)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to visualize topics using WordCloud\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    \n",
        "    # Function to create and display wordcloud\n",
        "    def display_wordcloud(topic_words, title):\n",
        "        wordcloud = WordCloud(\n",
        "            background_color='white',\n",
        "            width=800,\n",
        "            height=400,\n",
        "            max_words=100\n",
        "        ).generate(' '.join(topic_words))\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "    \n",
        "    # Display LDA topics\n",
        "    for idx, topic in enumerate(lda_topics):\n",
        "        display_wordcloud(topic, f'LDA Topic {idx+1}')\n",
        "    \n",
        "    # Display NMF topics\n",
        "    for idx, topic in enumerate(nmf_topics):\n",
        "        display_wordcloud(topic, f'NMF Topic {idx+1}')\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"WordCloud package not installed, skipping topic visualization\")\n",
        "    print(\"You can install it using: pip install wordcloud\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interpret and provide suggested topic names\n",
        "print(\"\\nTopic Interpretation and Suggested Names:\")\n",
        "print(\"\\nLDA Topics:\")\n",
        "for i, topic_words in enumerate(lda_topics):\n",
        "    print(f\"Topic {i+1}: {', '.join(topic_words)}\")\n",
        "    # The naming would be done manually in a real analysis\n",
        "    print(\"Suggested name: Requires human interpretation based on word patterns\\n\")\n",
        "\n",
        "print(\"\\nNMF Topics:\")\n",
        "for i, topic_words in enumerate(nmf_topics):\n",
        "    print(f\"Topic {i+1}: {', '.join(topic_words)}\")\n",
        "    # The naming would be done manually in a real analysis\n",
        "    print(\"Suggested name: Requires human interpretation based on word patterns\\n\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Conclusion and Summary\n",
        "\n",
        "In this notebook, we've explored a comprehensive NLP pipeline for news headlines analysis:\n",
        "\n",
        "1. **Text Preprocessing**:\n",
        "   - Implemented tokenization, lowercasing, stopword removal, and lemmatization\n",
        "   - Demonstrated how these steps transform the text and prepare it for analysis\n",
        "\n",
        "2. **Vectorization Methods**:\n",
        "   - Compared Bag of Words and TF-IDF vectorization\n",
        "   - Showed how these methods represent the same text differently\n",
        "\n",
        "3. **Sentiment Analysis**:\n",
        "   - Used TextBlob and VADER for sentiment classification\n",
        "   - Found differences between the two approaches and why VADER is generally better for short texts like headlines\n",
        "\n",
        "4. **Topic Modeling**:\n",
        "   - Applied LDA and NMF for discovering topics in the headlines\n",
        "   - Visualized topics and discussed differences between the two approaches\n",
        "\n",
        "### Key Findings:\n",
        "- Preprocessing is crucial for quality NLP results\n",
        "- TF-IDF generally provides better representations for topic modeling than simple count-based methods\n",
        "- VADER tends to be more accurate for short texts like headlines\n",
        "- NMF topics are often more interpretable than LDA topics for short texts\n",
        "\n",
        "This analysis demonstrates how different NLP techniques can extract insights from textual data, helping to understand the underlying patterns and sentiments in news coverage.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
